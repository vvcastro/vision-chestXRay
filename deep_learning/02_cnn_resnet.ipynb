{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PbK5GUAeWTZk",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 02:04:39.290920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "MASTER_FILE = os.path.join( '..', 'data', 'training_set.csv' )\n",
    "DATA_DIR = os.path.join( '..', 'data', 'train' )\n",
    "RESULTS_DIR = os.path.join( 'results' )\n",
    "\n",
    "## PARAMS\n",
    "IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTDQ-0gdiST2"
   },
   "source": [
    "## 01. Data Management:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c46SWnB_gZw"
   },
   "source": [
    "Load the master file with the information per patient:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1674409524440,
     "user": {
      "displayName": "Vicente Castro",
      "userId": "02943233680137416518"
     },
     "user_tz": -60
    },
    "id": "pGj4-OnpBbws",
    "outputId": "c67360b9-ddd5-4873-caf0-cc4d87e8690c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients: 12086\n"
     ]
    }
   ],
   "source": [
    "master_data = pd.read_csv( MASTER_FILE )[ ['file', 'label', 'patient_id'] ]\n",
    "master_data = master_data.groupby('patient_id').agg({'file': list, 'label': lambda x: np.unique(x)[0] })\n",
    "master_data = master_data.to_dict(orient='index')\n",
    "print(\"Number of patients:\", len(master_data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7snz7luOIylH"
   },
   "source": [
    "Write a custom pipeline to load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1HqWgraAM6a"
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = np.array( ['N', 'P', 'T'] )\n",
    "autotune = tf.data.AUTOTUNE\n",
    "\n",
    "def get_data_from_ids( split_ids ):\n",
    "    \"\"\" Get master files for a given set of ids \"\"\"\n",
    "    split_data = []\n",
    "    for split_id in split_ids:\n",
    "        for file_id in master_data[split_id]['file']:\n",
    "            file_path = os.path.join( DATA_DIR, file_id )\n",
    "            split_data.append( f\"{master_data[split_id]['label']}-{file_path}\" )\n",
    "    return split_data\n",
    "\n",
    "def load_image( image_path, img_size=(IMAGE_SIZE, IMAGE_SIZE)):\n",
    "    \"\"\" Loads a random image for the patient\"\"\"\n",
    "    img = tf.io.read_file( image_path )\n",
    "    img = tf.io.decode_image( img, channels=3, expand_animations=False )\n",
    "    img = tf.image.resize( img, img_size )\n",
    "    return img\n",
    "\n",
    "def preprocess_patient_path( patient_data ):\n",
    "    ''' Load patient image and patient label'''\n",
    "    parts = tf.strings.split( patient_data, '-')\n",
    "    \n",
    "    # get label\n",
    "    one_hot = parts[0] == CLASS_NAMES\n",
    "    patient_label = tf.cast(one_hot, dtype=tf.int8)\n",
    "\n",
    "    # get image\n",
    "    patient_image = load_image( parts[1] )\n",
    "    return patient_image, patient_label\n",
    "\n",
    "def define_dataset(train_ids, valid_ids):\n",
    "    \"\"\" Init and preprocess the datasets \"\"\"\n",
    "\n",
    "    # get master data into the train and valid data\n",
    "    train_data = get_data_from_ids(train_ids)\n",
    "    val_data = get_data_from_ids(valid_ids)\n",
    "\n",
    "    # load the labels file\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices( train_data )\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices( val_data )\n",
    "\n",
    "    # add shuffling\n",
    "    train_ds = train_ds.shuffle(buffer_size=len(train_data), reshuffle_each_iteration=True )\n",
    "    val_ds = val_ds.shuffle(buffer_size=len(val_data), reshuffle_each_iteration=True )\n",
    "\n",
    "    # add the patient data to the datasets\n",
    "    train_ds = train_ds.map( preprocess_patient_path, num_parallel_calls=autotune)\n",
    "    val_ds = val_ds.map( preprocess_patient_path, num_parallel_calls=autotune)\n",
    "    return train_ds, val_ds\n",
    "\n",
    "def visualize_data_samples( dataset, sample_size=3):\n",
    "    \"\"\" Visualize samples of the set\"\"\"\n",
    "    fig, ax = plt.subplots(1, sample_size, figsize=(15, 5))\n",
    "    for i, (sample_image, sample_label) in enumerate( dataset.take(sample_size) ):\n",
    "        ax[i].imshow(sample_image / 255 )\n",
    "        ax[i].set_title( CLASS_NAMES[np.argmax(sample_label.numpy())] )\n",
    "    plt.show();\n",
    "    return None\n",
    "\n",
    "\n",
    "#### DEFINE SOME VARIABLES\n",
    "X_data = np.array( list(master_data.keys()) )\n",
    "Y_data = np.array( [ master_data[pid]['label'] for pid in X_data ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "718eIgakiV4m"
   },
   "source": [
    "## 02. Metrics and Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75tdtERTifOY"
   },
   "source": [
    "Custom metrics we are going to track:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5WWimTRiZsl"
   },
   "outputs": [],
   "source": [
    "def macro_f1(y_true, y_pred):\n",
    "    \"\"\" For how the data is organized\"\"\"\n",
    "\n",
    "    def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v7lBsF4ih9B"
   },
   "source": [
    "Full evaluation of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GEYcY99iklc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "def evaluate_model(dataset, model):\n",
    "    \"\"\" \n",
    "    Calculate the main stats for the model:\n",
    "    - F1 Score (for each class)\n",
    "    - ACC Score (for each class)\n",
    "    - Confusion Matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # get the results for the whole dataset\n",
    "    y_true, y_pred = [], []\n",
    "    for batch_images, batch_labels in dataset:\n",
    "        model_outputs = tf.argmax( cnn_model( batch_images ), axis=1)\n",
    "        true_values = tf.argmax( batch_labels, axis=1)\n",
    "        y_pred.append( model_outputs )\n",
    "        y_true.append( true_values )\n",
    "    y_true = tf.concat(y_true, axis=0).numpy()\n",
    "    y_pred = tf.concat(y_pred, axis=0).numpy()\n",
    "\n",
    "    # calculate statistics for the data\n",
    "    f1_scores = f1_score( y_true=y_true, y_pred=y_pred, average=None)\n",
    "    conf_matrix = confusion_matrix( y_true=y_true, y_pred=y_pred, normalize='true' )\n",
    "\n",
    "    # compute acc for each class\n",
    "    normalized_cm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    acc_scores = normalized_cm.diagonal()\n",
    "    \n",
    "    # show the metrics\n",
    "    print( \"F1 Scores:\", {CLASS_NAMES[i]: f1_scores[i] for i in range(len(CLASS_NAMES))} )\n",
    "    print( \"Acc Scores:\", {CLASS_NAMES[i]: acc_scores[i] for i in range(len(CLASS_NAMES))} )\n",
    "    print()\n",
    "\n",
    "    # confusion matrix display\n",
    "    df_cm = pd.DataFrame(conf_matrix, index = [i for i in CLASS_NAMES], columns = [i for i in CLASS_NAMES])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(df_cm, annot=True)\n",
    "    plt.show();\n",
    "\n",
    "    # create the results data\n",
    "    results = {}\n",
    "    for metric_name, metric_values in zip( ['f1', 'acc'], [f1_scores, acc_scores] ):\n",
    "        for i, class_name in enumerate(CLASS_NAMES):\n",
    "            results[ f'{metric_name}_{class_name}'] = metric_values[i]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pInm5Jt4GAdI"
   },
   "source": [
    "## 03. Model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpIsGDMzWWKW"
   },
   "outputs": [],
   "source": [
    "def build_cnn_model( n_head_layers, num_classes=3 ):\n",
    "    \n",
    "    # preprocess backbone model\n",
    "    preprocessing_block = tf.keras.Sequential([\n",
    "        layers.Flatten(),\n",
    "        layers.BatchNormalization()\n",
    "    ], name='ConvProcessing')\n",
    "    \n",
    "    # define a classification head\n",
    "    head_layers = []\n",
    "    for i in range(n_head_layers):\n",
    "        head_block = tf.keras.Sequential([\n",
    "            layers.Dense(2**(6 + i), activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.BatchNormalization(),\n",
    "        ])\n",
    "        head_layers.append( head_block )\n",
    "    head_layers = head_layers[::-1]\n",
    "    head_layers.append( layers.Dense( num_classes ) )\n",
    "    classification_head = tf.keras.Sequential(head_layers, name='ClassHead')\n",
    "    \n",
    "    # build the model\n",
    "    input_image = layers.Input([IMAGE_SIZE, IMAGE_SIZE, 3], name='InputLayer')\n",
    "    x = layers.Lambda( keras.applications.resnet50.preprocess_input, name='Preprocessing')(input_image)\n",
    "    \n",
    "    # add resnet backbone\n",
    "    x = keras.applications.ResNet50(\n",
    "        input_tensor = x,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    ).output\n",
    "    x = preprocessing_block(x)\n",
    "    output_label = classification_head(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[input_image], outputs=[output_label], name='ResNetBackbone')\n",
    "\n",
    "def config_model_for_head_training( model ):\n",
    "    \"\"\" Freezes the convolutional layers of the backbone\"\"\"\n",
    "    for layer in model.layers[2].layers:\n",
    "        layer.trainable = False\n",
    "    return model\n",
    "\n",
    "def config_model_for_fine_tuning( n_unfrozen_layers, model ):\n",
    "    \"\"\" Unfreezes the last part of the conv backbone \"\"\"\n",
    "    for layer in model.layers[2].layers[-n_unfrozen_layers:]:\n",
    "        layer.trainable = True\n",
    "    return model\n",
    "\n",
    "def plot_model_training_hist( history ):\n",
    "    # plot the training history\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # plot accuracies\n",
    "    ax[0].set_title('Accuracy Metric')\n",
    "    ax[0].plot( history.history['accuracy'], label='Train' )\n",
    "    ax[0].plot(history.history['val_accuracy'], label='Valid')\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(loc='best')\n",
    "\n",
    "    # plot f1 scores\n",
    "    ax[1].set_title('F1-Score Metric')\n",
    "    ax[1].plot( history.history['macro_f1'], label='Train' )\n",
    "    ax[1].plot(history.history['val_macro_f1'], label='Valid')\n",
    "    ax[1].set_ylabel('F1-Score')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(loc='best')\n",
    "    plt.show()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9E-8IPxRaIj"
   },
   "source": [
    "\n",
    "## 04. Training:\n",
    "Cross validation for the patients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "148lqDX5d_GVQDVDuOeVDDYoBs7m2TSkg"
    },
    "executionInfo": {
     "elapsed": 4600158,
     "status": "ok",
     "timestamp": 1674414124592,
     "user": {
      "displayName": "Vicente Castro",
      "userId": "02943233680137416518"
     },
     "user_tz": -60
    },
    "id": "Z350jiPyRZpf",
    "outputId": "14fdf1fd-6b1f-4a5e-d153-e5dd634e006d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "\n",
    "LOSS_FUNCT_TO_USE = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "METRICS_TO_TRACK = [ 'accuracy', macro_f1 ]\n",
    "\n",
    "## TRAINING PARAMS\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "## TRAINING PHASES\n",
    "EPOCHS_HEAD_TRAINING = 10\n",
    "EPOCHS_FINE_TUNING = 5\n",
    "\n",
    "## MODEL PARAMS\n",
    "HEAD_SIZES = [ 2, 3, 4 ]\n",
    "UNFROZEN_LAYERS = [ 5, 10 ]\n",
    "\n",
    "## RESULTS\n",
    "training_results = []\n",
    "\n",
    "for n_head_layers, n_unfrozen_layers in product(HEAD_SIZES, UNFROZEN_LAYERS):\n",
    "    param_results = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=3)\n",
    "    for i, (train_idx, valid_idx) in enumerate(skf.split(X_data, Y_data)):\n",
    "        callback_head = keras.callbacks.EarlyStopping('val_macro_f1', mode='max', restore_best_weights=True, verbose=1, patience=2)\n",
    "        callback_fine = keras.callbacks.EarlyStopping('val_macro_f1', mode='max', restore_best_weights=True, verbose=1, patience=2)\n",
    "\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"\\t Head size: {n_head_layers} ; Unfrozen Layer: {n_unfrozen_layers} || Training for Fold\", i+1)\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "        # get the ids and build the datasets\n",
    "        train_ids, valid_ids = X_data[train_idx], X_data[valid_idx]\n",
    "        train_ds, valid_ds = define_dataset(train_ids, valid_ids)\n",
    "    \n",
    "        # visualize samples\n",
    "        visualize_data_samples( train_ds )\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "        # batch data\n",
    "        train_ds = train_ds.batch(BATCH_SIZE)\n",
    "        valid_ds = valid_ds.batch(BATCH_SIZE)\n",
    "\n",
    "        # compute the class_weights in the training data\n",
    "        Y_train = Y_data[train_idx]\n",
    "        train_weigths = class_weight.compute_class_weight( 'balanced', classes=CLASS_NAMES, y=Y_train )\n",
    "        train_weigths_dict = dict(enumerate(train_weigths)) \n",
    "\n",
    "        # build the model\n",
    "        cnn_model = build_cnn_model( n_head_layers, num_classes=3 )\n",
    "\n",
    "        with tf.device('/device:GPU:0'):\n",
    "    \n",
    "            # Training ---- Part 1: Training Classification Head\n",
    "            print( \" - Training Classification Head...\" )\n",
    "            cnn_model = config_model_for_head_training( cnn_model )\n",
    "            cnn_model.compile(optimizer='adam', loss=LOSS_FUNCT_TO_USE, metrics=METRICS_TO_TRACK)\n",
    "        \n",
    "            history_head = cnn_model.fit(\n",
    "                train_ds,\n",
    "                validation_data=valid_ds,\n",
    "                epochs=EPOCHS_HEAD_TRAINING,\n",
    "                class_weight=train_weigths_dict,\n",
    "                callbacks=[callback_head]\n",
    "            )\n",
    "            plot_model_training_hist(history_head)\n",
    "            print(\"=\" * 100)\n",
    "        \n",
    "            # Training ---- Part 2: Fine Tuning\n",
    "            print( \" - Training Fine Tuning...\" )\n",
    "            cnn_model = config_model_for_fine_tuning( n_unfrozen_layers, cnn_model )\n",
    "            cnn_model.compile(optimizer='adam', loss=LOSS_FUNCT_TO_USE, metrics=METRICS_TO_TRACK)\n",
    "        \n",
    "            history_fine = cnn_model.fit(\n",
    "                train_ds,\n",
    "                validation_data=valid_ds,\n",
    "                epochs=EPOCHS_FINE_TUNING,\n",
    "                class_weight=train_weigths_dict,\n",
    "                callbacks=[callback_fine]\n",
    "            )\n",
    "    \n",
    "            plot_model_training_hist(history_fine)\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "        # get and store the results\n",
    "        fold_results = evaluate_model(valid_ds, cnn_model)\n",
    "        fold_best_epoch_head = np.argmax( history_head.history['val_macro_f1'] ) + 1\n",
    "        fold_best_epoch_fine = np.argmax( history_fine.history['val_macro_f1'] ) + 1\n",
    "        fold_results['best_epoch_head'] = fold_best_epoch_head\n",
    "        fold_results['best_epoch_fine'] = fold_best_epoch_fine\n",
    "\n",
    "        param_results.append(fold_results)\n",
    "        del cnn_model, callback_head, callback_fine\n",
    "\n",
    "    # calculate the training results\n",
    "    param_results = pd.DataFrame.from_records(param_results)\n",
    "    cv_results = param_results.mean(axis=0).to_dict()\n",
    "\n",
    "    # add the parameters / \n",
    "    cv_results['head_layers'] = n_head_layers\n",
    "    cv_results['unfrozen_layers'] = n_unfrozen_layers\n",
    "    training_results.append( cv_results )\n",
    "\n",
    "training_results = pd.DataFrame.from_records( training_results )\n",
    "training_results.to_csv( os.path.join(RESULTS_DIR, 'resnet_cnn.csv') )\n",
    "\n",
    "training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_67GltgZsGl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
