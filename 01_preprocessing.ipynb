{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ae98fe-92df-45f9-a207-05e66902768a",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Here we present the pre-processing done to the dataset. We have basically two relevant task, that will change slightly depending on the ML method ( `feature-based` or `deep-learning` ) we are using. The basic steps are the following:\n",
    "\n",
    "1) Understand the cleaning of the dataset ( jut conceptually ).\n",
    "2) **Train/Test split** - here we need to make splits by patients, also being aware of class imbalances.\n",
    "3) Apply cleaning based on **training set**.\n",
    "4) Create a **processing pipeline**, we have two ways to apply the pipeline ( `on-loading` or storing the images ). We will check what work best for our case.\n",
    "\n",
    "**Note 1:** For number (4) it could make sense to have a `DatasetClass` able to apply _different pipelines_. It could make sense to apply this `on-loading` as it wouldn't be necessary to store and move the files to other environment ( if the models are trained in Colab or other services )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ddcd53f-eaa8-4a88-b785-241faa5681e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('data', 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc30d74-7a67-453d-a292-73782629bc27",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Cleaning Idea\n",
    "\n",
    "It is relevant to say that the previously defined `perturbation` are, in practice, nothing more than a kind of `data augmentation`. This is a really helpful process for Machine Learning algorithms and, given this, is something we want to use.\n",
    "\n",
    "We have two ways to do this (**note:** we want to develop a process useful for the three different classification approaches we are using ):\n",
    "\n",
    "1) We keep the dataset as it is ( with some minor preprocessing on top ) and, if wanted, apply _data augmentation techniques_ on top of it. -> This could be closer to the actual `test dataset`, so it could be benefitial to our performance.\n",
    "\n",
    "2) We clean the dataset to ideally have only \"good images\" and apply _data augmentation_ on top of it. This could allow us to perform more \"aggresive\" augmentation techniques ( as we start from a \"un-perturbed\" image ). Also, if the cleaning is not \"perfect\" it wouldn't be a real problem (as it would be a small augmentation) -> We would have to clean each `test` sample too.\n",
    "\n",
    "**Idea:** We will develop a cleaning idea, but ultimatly we will test both pipelines to check what yields better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2dd4ef-98a3-4fbf-8ed6-29143bf7005a",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split\n",
    "\n",
    "Ideally we want to define a unique `training` and `testing` datasets to compare in a un-biased way the methods we are developing. \n",
    "\n",
    "* As we are working with patients that can have more than one image, we have to split by patient ids.\n",
    "* As we are working with an imbalanced dataset, we have to split taking into account the stratify of the datasets ( both sets keep the same balance )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2683c247-4dc2-4269-8e78-d8bae9483de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 15470\n",
      "Total number of distinct patients: 12086\n"
     ]
    }
   ],
   "source": [
    "labels_file = pd.read_csv( os.path.join(BASE_DATA_DIR, 'labels_train.csv'))\n",
    "labels_file['patient_id'] = labels_file['file'].str.split('_').str[0]\n",
    "print(\"Total number of images:\", len(labels_file) )\n",
    "\n",
    "# drop file column and drop duplicates\n",
    "patients_file = labels_file.drop(['file'], axis=1)\n",
    "patients_file = patients_file.drop_duplicates(subset=['patient_id']).reset_index(drop=True)\n",
    "\n",
    "# show head and stats\n",
    "patients_file.head()\n",
    "print(\"Total number of distinct patients:\", len(patients_file) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e57e0c-9845-49ee-81e6-8a6808333cdb",
   "metadata": {},
   "source": [
    "Now we do the split based on the `patient_id`, take into account the class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17270ab5-f089-4903-b9f1-0e2687220a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: {'N': 0.6057421810359093, 'P': 0.27345689227205033, 'T': 0.12080092669204037}\n"
     ]
    }
   ],
   "source": [
    "classes, counts = np.unique(patients_file['label'], return_counts=True)\n",
    "original_dist = { classes[i]: counts[i] / len(patients_file) for i in range(len(classes)) }\n",
    "print(\"Original class distribution:\", original_dist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca7b2579-dc33-4466-888d-0f7eba085bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 12386 samples\n",
      "Testing data size: 3084 samples\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 0.80\n",
    "\n",
    "# make the actual split\n",
    "training_ids, testing_ids = train_test_split(\n",
    "    patients_file['patient_id'].values,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    stratify=patients_file['label'].values\n",
    ")\n",
    "training_data = labels_file[ labels_file['patient_id'].isin( training_ids )]\n",
    "testing_data = labels_file[ labels_file['patient_id'].isin( testing_ids )]\n",
    "print(\"Training data size:\", len(training_data), \"samples\" )\n",
    "print(\"Testing data size:\", len(testing_data), \"samples\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6a3e6-ffdd-44bd-9cf0-848e29f461e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
