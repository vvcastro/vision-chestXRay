{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277117b3-60c1-4121-88b8-b724acff6d50",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "In this file we have the validation and training for the `Feature Based` algorithms we are developing. Specifically, you can find:\n",
    "\n",
    "- Cross Validation splits\n",
    "- Feature Reduction / Feature selection ( by groups )\n",
    "- Models (SVM and RandomForest)\n",
    "\n",
    "It's relevant to understand that the proposed pipeline tries to validate the `models` we are implementing and how the different subsets/techniques change the this performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb7e6d-e5a8-4843-98ef-9ca90828a532",
   "metadata": {},
   "source": [
    "## 0. Relevant functions:\n",
    "\n",
    "Here we set the `cross_validation` idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b245925d-d161-4302-b12a-7c24e60d81d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# define basic files\n",
    "FEATURES_DIR = os.path.join('..', 'data', 'features')\n",
    "TRAIN_CSV_FILE = os.path.join('..', 'data', 'training_set.csv')\n",
    "TEST_CSV_FILE = os.path.join('..', 'data', 'testing_set.csv')\n",
    "\n",
    "# load the train data label\n",
    "train_file = pd.read_csv(TRAIN_CSV_FILE)\n",
    "test_file = pd.read_csv(TEST_CSV_FILE)\n",
    "\n",
    "def load_subset_features( data, subset ):\n",
    "    \"\"\" We load the pre-stored features\"\"\"\n",
    "    features = np.load( os.path.join( FEATURES_DIR, subset_name + '.npy'), allow_pickle=True )[()]\n",
    "    return [ features[fname] for fname in data['file'] ]\n",
    "\n",
    "def generate_group_based_splits( data, n_splits=5, random_state=0):\n",
    "    '''\n",
    "    Uses the 'tag' column from train_data and makes `n_plists` stratified splits.\n",
    "    Stratified -> balanced according to the labels\n",
    "    Parameters:\n",
    "        data (dataframe): with `tag` columns for `patient_id`.\n",
    "    Returns:\n",
    "        generator: yields the train and test indices\n",
    "    '''\n",
    "    stratified_kfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    unique_tag_data = data.drop_duplicates(subset=['patient_id'], keep='first')\n",
    "    return stratified_kfolds.split( unique_tag_data['patient_id'], unique_tag_data['label'] )\n",
    "\n",
    "def get_fold_features( train_fold, valid_fold, subset_name):\n",
    "    \"\"\" Load the extracted features, from the pre-stored files \"\"\"\n",
    "    features = np.load( os.path.join( FEATURES_DIR, subset_name + '.npy'), allow_pickle=True )[()]\n",
    "    train_features = [ features[fname] for fname in train_fold['file'] ]\n",
    "    valid_features = [ features[fname] for fname in valid_fold['file'] ]\n",
    "    return train_features, valid_features\n",
    "\n",
    "def get_raw_features( fold_data ):\n",
    "    \"\"\" \n",
    "    Stack the features for a full-training on the features.\n",
    "    We just need to concatenate the features for each sample.\n",
    "    \"\"\"\n",
    "    fts_order = [ 'hu_bin', 'fourier_real', 'fourier_imag', 'stats', 'gabor', 'lbp', 'haralick' ]\n",
    "    data_fts = [ np.concatenate([ sample[ftname] for ftname in fts_order] ) for sample in fold_data ]\n",
    "    data_fts = np.stack( data_fts )\n",
    "    data_fts[ np.isnan(data_fts) ] = 0\n",
    "    return data_fts\n",
    "\n",
    "def find_elbow(data):\n",
    "    cumulative_variance = np.cumsum( data ) / np.sum(data)\n",
    "    return np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "def apply_group_pca( tfold_data, vfold_data ):\n",
    "    \"\"\" Group the features by type, calculate the optimal PCA compression and transform each group \"\"\"\n",
    "    fts_geo, fts_int, fts_text = [ 'hu_bin', 'fourier_real', 'fourier_imag'], [ 'stats', 'gabor' ], ['lbp', 'haralick' ]\n",
    "    \n",
    "    compressed_tdata, compressed_vdata = [], []\n",
    "    for fts_group in [ fts_geo, fts_int, fts_text ]:\n",
    "        tdata_fts = [ np.concatenate([ sample[ftname] for ftname in fts_group] ) for sample in tfold_data ]\n",
    "        vdata_fts = [ np.concatenate([ sample[ftname] for ftname in fts_group] ) for sample in vfold_data ]\n",
    "        tdata_fts, vdata_fts = np.stack( tdata_fts ), np.stack( vdata_fts )\n",
    "        tdata_fts[ np.isnan(tdata_fts) ] = 0\n",
    "        vdata_fts[ np.isnan(vdata_fts) ] = 0\n",
    "        \n",
    "        tdata_norm = StandardScaler().fit_transform( tdata_fts )\n",
    "        vdata_norm = StandardScaler().fit_transform( vdata_fts )\n",
    "        \n",
    "        # study PCA\n",
    "        pca = PCA()\n",
    "        pca_result = pca.fit_transform( tdata_norm )\n",
    "        optimal_components = find_elbow(pca.explained_variance_ratio_)\n",
    "        \n",
    "        # apply PCA\n",
    "        tdata_pca = PCA(n_components=optimal_components).fit_transform( tdata_norm )\n",
    "        vdata_pca = PCA(n_components=optimal_components).fit_transform( vdata_norm )\n",
    "        compressed_tdata.append(tdata_pca), compressed_vdata.append(vdata_pca)\n",
    "    return np.concatenate( compressed_tdata, axis=1), np.concatenate( compressed_vdata, axis=1)\n",
    "\n",
    "def apply_group_thresholding( tfold_data, vfold_data, thresh=0.1 ):\n",
    "    \"\"\" \n",
    "    Group the features by type and remove, by threhsolding, the features with low-variance\n",
    "    \"\"\"\n",
    "    fts_geo, fts_int, fts_text = [ 'hu_bin', 'fourier_real', 'fourier_imag'], [ 'stats', 'gabor' ], ['lbp', 'haralick' ]\n",
    "    \n",
    "    reduced_tdata, reduced_vdata = [], []\n",
    "    for fts_group in [ fts_geo, fts_int, fts_text ]:\n",
    "        tdata_fts = [ np.concatenate([ sample[ftname] for ftname in fts_group] ) for sample in tfold_data ]\n",
    "        vdata_fts = [ np.concatenate([ sample[ftname] for ftname in fts_group] ) for sample in vfold_data ]\n",
    "        tdata_fts, vdata_fts = np.stack( tdata_fts ), np.stack( vdata_fts )\n",
    "        tdata_fts[ np.isnan(tdata_fts) ] = 0\n",
    "        vdata_fts[ np.isnan(vdata_fts) ] = 0\n",
    "\n",
    "        # study variance thresholding\n",
    "        thresholding = VarianceThreshold(threshold=thresh)\n",
    "        tdata_reduced = thresholding.fit_transform( tdata_fts )\n",
    "        vdata_reduced = thresholding.transform( vdata_fts )\n",
    "        reduced_tdata.append(tdata_reduced), reduced_vdata.append(vdata_reduced)\n",
    "    return np.concatenate( reduced_tdata, axis=1), np.concatenate( reduced_vdata, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb839ab-e087-4b60-a382-1c979d78ee42",
   "metadata": {},
   "source": [
    "# 1. Support Vector Machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b231216-579a-48e6-8808-2931cb2ce589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "DATA_SUBSETS = [ \n",
    "    'small_without_noise_not_inverted', 'small_with_noise_inverted', 'medium_with_noise_inverted'\n",
    "]\n",
    "\n",
    "# gridSearch for SVM\n",
    "param_grid = {\n",
    "    'C': [5, 10, 100], \n",
    "    'gamma': ['scale', 0.001],\n",
    "    'kernel': ['poly', 'rbf'],\n",
    "    'subset': DATA_SUBSETS\n",
    "}\n",
    "\n",
    "combinations = list( product(*param_grid.values()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c37e45-7551-465a-9413-2d09af83c6a7",
   "metadata": {},
   "source": [
    "GridSearch for the optimal results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23f67d9d-145c-4bf5-8703-f2baa01a9036",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing combination: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [32:45<00:00, 109.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "\n",
    "# make all the combinations for the experiments\n",
    "svm_results = []\n",
    "for pvalues in tqdm( combinations, desc='Testing combination' ):\n",
    "    svm_c, svm_gamma, svm_kernel = pvalues[0], pvalues[1], pvalues[2]\n",
    "    data_subset = pvalues[3]\n",
    "    \n",
    "    fold_generator = generate_group_based_splits( train_file, n_splits=3 )\n",
    "    combination_results = { 'raw': [], 'pca': [], 'var': [] }\n",
    "    for tfold, vfold in fold_generator:\n",
    "        \n",
    "        tfold_data, vfold_data = train_file.iloc[tfold], train_file.iloc[vfold]\n",
    "        tfold_Y, vfold_Y = tfold_data['label'].values, vfold_data['label'].values\n",
    "        tfold_fts, vfold_fts = get_fold_features( tfold_data, vfold_data, subset_name=data_subset )\n",
    "\n",
    "        # raw features\n",
    "        tfold_fts_raw = get_raw_features( tfold_fts )\n",
    "        vfold_fts_raw = get_raw_features( vfold_fts )\n",
    "\n",
    "        # pca/thresholding features\n",
    "        tfold_fts_pca, vfold_fts_pca = apply_group_pca( tfold_fts, vfold_fts )\n",
    "        tfold_fts_var, vfold_fts_var = apply_group_thresholding( tfold_fts, vfold_fts )\n",
    "        \n",
    "        ### Raw Testing\n",
    "        clf = make_pipeline( StandardScaler(), SVC(C=svm_c, gamma=svm_gamma, kernel=svm_kernel, class_weight='balanced') )\n",
    "        clf.fit( tfold_fts_raw, tfold_Y )\n",
    "        vfold_raw_pred = clf.predict( vfold_fts_raw )\n",
    "\n",
    "        raw_acc = clf.score( vfold_fts_raw, vfold_Y )\n",
    "        raw_f1 = f1_score( vfold_Y, vfold_raw_pred, average=None )[ np.where(clf[1].classes_ == 'T')[0][0] ]\n",
    "        raw_f1_macro = f1_score( vfold_Y, vfold_raw_pred, average='macro') # overall, unweighted metric\n",
    "        combination_results['raw'].append( (raw_acc, raw_f1, raw_f1_macro) )\n",
    "        \n",
    "        ### Group-based PCA\n",
    "        clf = make_pipeline( SVC(C=svm_c, gamma=svm_gamma, kernel=svm_kernel, class_weight='balanced') )\n",
    "        clf.fit( tfold_fts_pca, tfold_Y )\n",
    "        vfold_pca_pred = clf.predict( vfold_fts_pca )\n",
    "\n",
    "        pca_acc = clf.score( vfold_fts_pca, vfold_Y )\n",
    "        pca_f1 = f1_score( vfold_Y, vfold_pca_pred, average=None )[ np.where(clf[0].classes_ == 'T')[0][0] ]\n",
    "        pca_f1_macro = f1_score(vfold_Y, vfold_pca_pred, average='macro')\n",
    "        combination_results['pca'].append( (pca_acc, pca_f1, pca_f1_macro) )\n",
    "        \n",
    "        ### Group-based Variance Thrhesholding\n",
    "        clf = make_pipeline( StandardScaler(), SVC(C=svm_c, gamma=svm_gamma, kernel=svm_kernel, class_weight='balanced') )\n",
    "        clf.fit( tfold_fts_var, tfold_Y )\n",
    "        vfold_var_pred = clf.predict( vfold_fts_var )\n",
    "\n",
    "        var_acc = clf.score( vfold_fts_var, vfold_Y )\n",
    "        var_f1 = f1_score( vfold_Y, vfold_var_pred, average=None )[ np.where(clf[1].classes_ == 'T')[0][0] ]\n",
    "        var_f1_macro = f1_score(vfold_Y, vfold_var_pred, average='macro')\n",
    "        combination_results['var'].append( (var_acc, var_f1, var_f1_macro) )\n",
    "        \n",
    "    for rtype in combination_results:\n",
    "        svm_results.append({\n",
    "            'C': svm_c, 'Gamma': svm_gamma, 'Kernel': svm_kernel,\n",
    "            'subset': data_subset,\n",
    "            'Ft_type': rtype, \n",
    "            'acc': np.median( [ v[0] for v in combination_results[rtype] ] ),\n",
    "            'f1_tub': np.median( [ v[1] for v in combination_results[rtype] ] ),\n",
    "            'f1_macro': np.median( [ v[2] for v in combination_results[rtype] ] )\n",
    "        })\n",
    "    \n",
    "svm_results = pd.DataFrame.from_records(svm_results)\n",
    "svm_results.to_csv('svm_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "644f7edc-29f9-4b27-ab9e-6eabd9bf9d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>Kernel</th>\n",
       "      <th>subset</th>\n",
       "      <th>Ft_type</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_tub</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rbf</td>\n",
       "      <td>small_with_noise_inverted</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.798635</td>\n",
       "      <td>0.517533</td>\n",
       "      <td>0.734064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>rbf</td>\n",
       "      <td>small_with_noise_inverted</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.789016</td>\n",
       "      <td>0.509271</td>\n",
       "      <td>0.720798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rbf</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.795158</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.729100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>scale</td>\n",
       "      <td>rbf</td>\n",
       "      <td>small_with_noise_inverted</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.797704</td>\n",
       "      <td>0.497382</td>\n",
       "      <td>0.723493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rbf</td>\n",
       "      <td>small_with_noise_inverted</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.759851</td>\n",
       "      <td>0.490414</td>\n",
       "      <td>0.706347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      C  Gamma Kernel                      subset Ft_type       acc    f1_tub  \\\n",
       "48  100  0.001    rbf   small_with_noise_inverted     raw  0.798635  0.517533   \n",
       "3     5  scale    rbf   small_with_noise_inverted     raw  0.789016  0.509271   \n",
       "51  100  0.001    rbf  medium_with_noise_inverted     raw  0.795158  0.509091   \n",
       "21   10  scale    rbf   small_with_noise_inverted     raw  0.797704  0.497382   \n",
       "30   10  0.001    rbf   small_with_noise_inverted     raw  0.759851  0.490414   \n",
       "\n",
       "    f1_macro  \n",
       "48  0.734064  \n",
       "3   0.720798  \n",
       "51  0.729100  \n",
       "21  0.723493  \n",
       "30  0.706347  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_results.sort_values(by=['f1_tub', 'f1_macro'], ascending=False).head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52e91e-4ed3-46de-b626-4c23e204b817",
   "metadata": {},
   "source": [
    "# 2. Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbb54a34-351a-488a-9852-e0115995690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "DATA_SUBSETS = [ \n",
    "    'small_without_noise_not_inverted', 'medium_without_noise_not_inverted', 'medium_with_noise_inverted', \n",
    "    # 'small_with_noise_not_inverted', 'small_without_noise_inverted'\n",
    "]\n",
    "\n",
    "# gridSearch for RandomForest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150], \n",
    "    'subset': DATA_SUBSETS\n",
    "}\n",
    "\n",
    "combinations = list( product(*param_grid.values()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4e1a7-2f9c-46ce-b158-b3cf03e0b1e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Perform the gridseach over the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca7e644-81b0-4ddf-9f56-d161b8ec490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing combination: 100%|██████████████████████████████████████████████████████████████████████████| 9/9 [14:09<00:00, 94.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_Estimators</th>\n",
       "      <th>Ft_type</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>Subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.796235</td>\n",
       "      <td>0.167748</td>\n",
       "      <td>0.611370</td>\n",
       "      <td>small_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.631982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352576</td>\n",
       "      <td>small_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>var</td>\n",
       "      <td>0.719487</td>\n",
       "      <td>0.087818</td>\n",
       "      <td>0.507867</td>\n",
       "      <td>small_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.801200</td>\n",
       "      <td>0.150520</td>\n",
       "      <td>0.614593</td>\n",
       "      <td>medium_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.648945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377692</td>\n",
       "      <td>medium_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>var</td>\n",
       "      <td>0.736657</td>\n",
       "      <td>0.081744</td>\n",
       "      <td>0.521406</td>\n",
       "      <td>medium_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.806165</td>\n",
       "      <td>0.246940</td>\n",
       "      <td>0.640821</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.632396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334099</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>var</td>\n",
       "      <td>0.750310</td>\n",
       "      <td>0.137463</td>\n",
       "      <td>0.555865</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.801820</td>\n",
       "      <td>0.152681</td>\n",
       "      <td>0.612894</td>\n",
       "      <td>small_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.633844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347605</td>\n",
       "      <td>small_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100</td>\n",
       "      <td>var</td>\n",
       "      <td>0.723211</td>\n",
       "      <td>0.088754</td>\n",
       "      <td>0.509450</td>\n",
       "      <td>small_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.805751</td>\n",
       "      <td>0.153460</td>\n",
       "      <td>0.616160</td>\n",
       "      <td>medium_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.644808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374176</td>\n",
       "      <td>medium_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100</td>\n",
       "      <td>var</td>\n",
       "      <td>0.738726</td>\n",
       "      <td>0.072653</td>\n",
       "      <td>0.522705</td>\n",
       "      <td>medium_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.808440</td>\n",
       "      <td>0.238673</td>\n",
       "      <td>0.641472</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.634257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337169</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100</td>\n",
       "      <td>var</td>\n",
       "      <td>0.752586</td>\n",
       "      <td>0.125942</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>150</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.799545</td>\n",
       "      <td>0.142303</td>\n",
       "      <td>0.609269</td>\n",
       "      <td>small_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>150</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.634878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352119</td>\n",
       "      <td>small_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>150</td>\n",
       "      <td>var</td>\n",
       "      <td>0.724038</td>\n",
       "      <td>0.070514</td>\n",
       "      <td>0.507273</td>\n",
       "      <td>small_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>150</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.806372</td>\n",
       "      <td>0.153399</td>\n",
       "      <td>0.621993</td>\n",
       "      <td>medium_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>150</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.653496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380989</td>\n",
       "      <td>medium_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>150</td>\n",
       "      <td>var</td>\n",
       "      <td>0.740174</td>\n",
       "      <td>0.073137</td>\n",
       "      <td>0.520490</td>\n",
       "      <td>medium_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>150</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.808647</td>\n",
       "      <td>0.244107</td>\n",
       "      <td>0.641193</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>150</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.637981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341016</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>150</td>\n",
       "      <td>var</td>\n",
       "      <td>0.756723</td>\n",
       "      <td>0.142924</td>\n",
       "      <td>0.562794</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    N_Estimators Ft_type       acc        f1  f1_macro  \\\n",
       "0             50     raw  0.796235  0.167748  0.611370   \n",
       "1             50     pca  0.631982  0.000000  0.352576   \n",
       "2             50     var  0.719487  0.087818  0.507867   \n",
       "3             50     raw  0.801200  0.150520  0.614593   \n",
       "4             50     pca  0.648945  0.000000  0.377692   \n",
       "5             50     var  0.736657  0.081744  0.521406   \n",
       "6             50     raw  0.806165  0.246940  0.640821   \n",
       "7             50     pca  0.632396  0.000000  0.334099   \n",
       "8             50     var  0.750310  0.137463  0.555865   \n",
       "9            100     raw  0.801820  0.152681  0.612894   \n",
       "10           100     pca  0.633844  0.000000  0.347605   \n",
       "11           100     var  0.723211  0.088754  0.509450   \n",
       "12           100     raw  0.805751  0.153460  0.616160   \n",
       "13           100     pca  0.644808  0.000000  0.374176   \n",
       "14           100     var  0.738726  0.072653  0.522705   \n",
       "15           100     raw  0.808440  0.238673  0.641472   \n",
       "16           100     pca  0.634257  0.000000  0.337169   \n",
       "17           100     var  0.752586  0.125942  0.554500   \n",
       "18           150     raw  0.799545  0.142303  0.609269   \n",
       "19           150     pca  0.634878  0.000000  0.352119   \n",
       "20           150     var  0.724038  0.070514  0.507273   \n",
       "21           150     raw  0.806372  0.153399  0.621993   \n",
       "22           150     pca  0.653496  0.000000  0.380989   \n",
       "23           150     var  0.740174  0.073137  0.520490   \n",
       "24           150     raw  0.808647  0.244107  0.641193   \n",
       "25           150     pca  0.637981  0.000000  0.341016   \n",
       "26           150     var  0.756723  0.142924  0.562794   \n",
       "\n",
       "                               Subset  \n",
       "0    small_without_noise_not_inverted  \n",
       "1    small_without_noise_not_inverted  \n",
       "2    small_without_noise_not_inverted  \n",
       "3   medium_without_noise_not_inverted  \n",
       "4   medium_without_noise_not_inverted  \n",
       "5   medium_without_noise_not_inverted  \n",
       "6          medium_with_noise_inverted  \n",
       "7          medium_with_noise_inverted  \n",
       "8          medium_with_noise_inverted  \n",
       "9    small_without_noise_not_inverted  \n",
       "10   small_without_noise_not_inverted  \n",
       "11   small_without_noise_not_inverted  \n",
       "12  medium_without_noise_not_inverted  \n",
       "13  medium_without_noise_not_inverted  \n",
       "14  medium_without_noise_not_inverted  \n",
       "15         medium_with_noise_inverted  \n",
       "16         medium_with_noise_inverted  \n",
       "17         medium_with_noise_inverted  \n",
       "18   small_without_noise_not_inverted  \n",
       "19   small_without_noise_not_inverted  \n",
       "20   small_without_noise_not_inverted  \n",
       "21  medium_without_noise_not_inverted  \n",
       "22  medium_without_noise_not_inverted  \n",
       "23  medium_without_noise_not_inverted  \n",
       "24         medium_with_noise_inverted  \n",
       "25         medium_with_noise_inverted  \n",
       "26         medium_with_noise_inverted  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# make all the combinations for the experiments\n",
    "rf_results = []\n",
    "for pvalues in tqdm( combinations, desc='Testing combination' ):\n",
    "    rf_estimators = pvalues[0]\n",
    "    data_subset = pvalues[1]\n",
    "    \n",
    "    fold_generator = generate_group_based_splits( train_file, n_splits=4 )\n",
    "    combination_results = { 'raw': [], 'pca': [], 'var': [] }\n",
    "    for tfold, vfold in fold_generator:\n",
    "        \n",
    "        tfold_data, vfold_data = train_file.iloc[tfold], train_file.iloc[vfold]\n",
    "        tfold_Y, vfold_Y = tfold_data['label'].values, vfold_data['label'].values\n",
    "        tfold_fts, vfold_fts = get_fold_features( tfold_data, vfold_data, subset_name=data_subset )\n",
    "\n",
    "        # raw features\n",
    "        tfold_fts_raw = get_raw_features( tfold_fts )\n",
    "        vfold_fts_raw = get_raw_features( vfold_fts )\n",
    "\n",
    "        # pca/thresholding features\n",
    "        tfold_fts_pca, vfold_fts_pca = apply_group_pca( tfold_fts, vfold_fts )\n",
    "        tfold_fts_var, vfold_fts_var = apply_group_thresholding( tfold_fts, vfold_fts )\n",
    "        \n",
    "        ### Raw Testing\n",
    "        clf = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=rf_estimators, class_weight='balanced'))\n",
    "        clf.fit( tfold_fts_raw, tfold_Y )\n",
    "        vfold_raw_pred = clf.predict( vfold_fts_raw )\n",
    "\n",
    "        raw_acc = clf.score( vfold_fts_raw, vfold_Y )\n",
    "        raw_f1 = f1_score( vfold_Y, vfold_raw_pred, average=None )[ np.where(clf[1].classes_ == 'T')[0][0] ]\n",
    "        raw_f1_macro = f1_score( vfold_Y, vfold_raw_pred, average='macro')\n",
    "        combination_results['raw'].append( (raw_acc, raw_f1, raw_f1_macro) )\n",
    "        \n",
    "        ### Group-based PCA\n",
    "        clf = make_pipeline( RandomForestClassifier(n_estimators=rf_estimators, class_weight='balanced'))\n",
    "        clf.fit( tfold_fts_pca, tfold_Y )\n",
    "        vfold_pca_pred = clf.predict( vfold_fts_pca )\n",
    "\n",
    "        pca_acc = clf.score( vfold_fts_pca, vfold_Y )\n",
    "        pca_f1 = f1_score( vfold_Y, vfold_pca_pred, average=None )[ np.where(clf[0].classes_ == 'T')[0][0] ]\n",
    "        pca_f1_macro = f1_score(vfold_Y, vfold_pca_pred, average='macro')\n",
    "        combination_results['pca'].append( (pca_acc, pca_f1, pca_f1_macro) )\n",
    "        \n",
    "        ### Group-based Variance Thrhesholding\n",
    "        clf = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=rf_estimators, class_weight='balanced'))\n",
    "        clf.fit( tfold_fts_var, tfold_Y )\n",
    "        vfold_var_pred = clf.predict( vfold_fts_var )\n",
    "\n",
    "        var_acc = clf.score( vfold_fts_var, vfold_Y )\n",
    "        var_f1 = f1_score( vfold_Y, vfold_var_pred, average=None )[ np.where(clf[1].classes_ == 'T')[0][0] ]\n",
    "        var_f1_macro = f1_score(vfold_Y, vfold_var_pred, average='macro')\n",
    "        combination_results['var'].append( (var_acc, var_f1, var_f1_macro) )\n",
    "        \n",
    "    for rtype in combination_results:\n",
    "        rf_results.append({\n",
    "            'N_Estimators': rf_estimators,\n",
    "            'Ft_type': rtype, \n",
    "            'acc': np.median( [ v[0] for v in combination_results[rtype] ] ),\n",
    "            'f1': np.median( [ v[1] for v in combination_results[rtype] ] ),\n",
    "            'f1_macro': np.median( [ v[2] for v in combination_results[rtype] ] ),\n",
    "            'Subset': data_subset,\n",
    "        })\n",
    "    \n",
    "rf_results = pd.DataFrame.from_records(rf_results)\n",
    "rf_results.to_csv('rf_results.csv', index=False)\n",
    "rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb7baec7-35d3-41ca-a283-34dd15987ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_Estimators</th>\n",
       "      <th>Ft_type</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>Subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.806165</td>\n",
       "      <td>0.246940</td>\n",
       "      <td>0.640821</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>150</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.808647</td>\n",
       "      <td>0.244107</td>\n",
       "      <td>0.641193</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.808440</td>\n",
       "      <td>0.238673</td>\n",
       "      <td>0.641472</td>\n",
       "      <td>medium_with_noise_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.796235</td>\n",
       "      <td>0.167748</td>\n",
       "      <td>0.611370</td>\n",
       "      <td>small_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100</td>\n",
       "      <td>raw</td>\n",
       "      <td>0.805751</td>\n",
       "      <td>0.153460</td>\n",
       "      <td>0.616160</td>\n",
       "      <td>medium_without_noise_not_inverted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    N_Estimators Ft_type       acc        f1  f1_macro  \\\n",
       "6             50     raw  0.806165  0.246940  0.640821   \n",
       "24           150     raw  0.808647  0.244107  0.641193   \n",
       "15           100     raw  0.808440  0.238673  0.641472   \n",
       "0             50     raw  0.796235  0.167748  0.611370   \n",
       "12           100     raw  0.805751  0.153460  0.616160   \n",
       "\n",
       "                               Subset  \n",
       "6          medium_with_noise_inverted  \n",
       "24         medium_with_noise_inverted  \n",
       "15         medium_with_noise_inverted  \n",
       "0    small_without_noise_not_inverted  \n",
       "12  medium_without_noise_not_inverted  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results.sort_values(by=['f1', 'f1_macro'], ascending=False).head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00be48b-eefb-49c6-b546-f634bc810451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
