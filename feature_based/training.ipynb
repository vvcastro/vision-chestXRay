{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277117b3-60c1-4121-88b8-b724acff6d50",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "In this file we have the validation and training for the `Feature Based` algorithms we are developing. Specifically, you can find:\n",
    "\n",
    "- Cross Validation splits\n",
    "- Feature Reduction / Feature selection ( by groups )\n",
    "- Models (SVM and RandomForest)\n",
    "\n",
    "It's relevant to understand that the proposed pipeline tries to validate the `models` we are implementing and how the different subsets/techniques change the this performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb7e6d-e5a8-4843-98ef-9ca90828a532",
   "metadata": {},
   "source": [
    "## 0. Relevant functions:\n",
    "\n",
    "Here we set the `cross_validation` idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b245925d-d161-4302-b12a-7c24e60d81d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# define basic files\n",
    "FEATURES_DIR = os.path.join('..', 'data', 'features')\n",
    "TRAIN_CSV_FILE = os.path.join('..', 'data', 'training_set.csv')\n",
    "TEST_CSV_FILE = os.path.join('..', 'data', 'testing_set.csv')\n",
    "\n",
    "# load the train data label\n",
    "train_file = pd.read_csv(TRAIN_CSV_FILE)\n",
    "test_file = pd.read_csv(TEST_CSV_FILE)\n",
    "\n",
    "def load_subset_features( data, subset ):\n",
    "    \"\"\" We load the pre-stored features\"\"\"\n",
    "    features = np.load( os.path.join( FEATURES_DIR, subset_name + '.npy'), allow_pickle=True )[()]\n",
    "    return [ features[fname] for fname in data['file'] ]\n",
    "\n",
    "def generate_group_based_splits( data, n_splits=5, random_state=0):\n",
    "    '''\n",
    "    Uses the 'tag' column from train_data and makes `n_plists` stratified splits.\n",
    "    Stratified -> balanced according to the labels\n",
    "    Parameters:\n",
    "        data (dataframe): with `tag` columns for `patient_id`.\n",
    "    Returns:\n",
    "        generator: yields the train and test indices\n",
    "    '''\n",
    "    stratified_kfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    unique_tag_data = data.drop_duplicates(subset=['patient_id'], keep='first')\n",
    "    return stratified_kfolds.split( unique_tag_data['patient_id'], unique_tag_data['label'] )\n",
    "\n",
    "def get_fold_features( train_fold, valid_fold, subset_name):\n",
    "    \"\"\" Load the extracted features, from the pre-stored files \"\"\"\n",
    "    features = np.load( os.path.join( FEATURES_DIR, subset_name + '.npy'), allow_pickle=True )[()]\n",
    "    train_features = [ features[fname] for fname in train_fold['file'] ]\n",
    "    valid_features = [ features[fname] for fname in valid_fold['file'] ]\n",
    "    return train_features, valid_features\n",
    "\n",
    "def get_raw_features( fold_data ):\n",
    "    \"\"\" \n",
    "    Stack the features for a full-training on the features.\n",
    "    We just need to concatenate the features for each sample.\n",
    "    \"\"\"\n",
    "    fts_order = [ 'hu_bin', 'fourier_real', 'fourier_imag', 'stats', 'gabor', 'lbp', 'haralick' ]\n",
    "    data_fts = [ np.concatenate([ sample[ftname] for ftname in fts_order] ) for sample in fold_data ]\n",
    "    data_fts = np.stack( data_fts )\n",
    "    data_fts[ np.isnan(data_fts) ] = 0\n",
    "    return data_fts\n",
    "\n",
    "def find_elbow(data):\n",
    "    cumulative_variance = np.cumsum( data ) / np.sum(data)\n",
    "    return np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "def apply_group_pca( tfold_data, vfold_data ):\n",
    "    \"\"\" Group the features by type, calculate the optimal PCA compression and transform each group \"\"\"\n",
    "    fts_geo, fts_int, fts_text = [ 'hu_bin', 'fourier_real', 'fourier_imag'], [ 'stats', 'gabor' ], ['lbp', 'haralick' ]\n",
    "    \n",
    "    compressed_tdata, compressed_vdata = [], []\n",
    "    for fts_group in [ fts_geo, fts_int, fts_text ]:\n",
    "        tdata_fts = [ np.concatenate([ sample[ftname] for ftname in fts_group] ) for sample in tfold_data ]\n",
    "        vdata_fts = [ np.concatenate([ sample[ftname] for ftname in fts_group] ) for sample in vfold_data ]\n",
    "        tdata_fts, vdata_fts = np.stack( tdata_fts ), np.stack( vdata_fts )\n",
    "        tdata_fts[ np.isnan(tdata_fts) ] = 0\n",
    "        vdata_fts[ np.isnan(vdata_fts) ] = 0\n",
    "        \n",
    "        tdata_norm = StandardScaler().fit_transform( tdata_fts )\n",
    "        vdata_norm = StandardScaler().fit_transform( vdata_fts )\n",
    "        \n",
    "        # study PCA\n",
    "        pca = PCA()\n",
    "        pca_result = pca.fit_transform( tdata_norm )\n",
    "        optimal_components = find_elbow(pca.explained_variance_ratio_)\n",
    "        \n",
    "        # apply PCA\n",
    "        tdata_pca = PCA(n_components=optimal_components).fit_transform( tdata_norm )\n",
    "        vdata_pca = PCA(n_components=optimal_components).fit_transform( vdata_norm )\n",
    "        compressed_tdata.append(tdata_pca), compressed_vdata.append(vdata_pca)\n",
    "    return np.concatenate( compressed_tdata, axis=1), np.concatenate( compressed_vdata, axis=1)\n",
    "\n",
    "def apply_group_thresholding( tfold_data, vfold_data, thresh=0.1 ):\n",
    "    \"\"\" \n",
    "    Group the features by type and remove, by threhsolding, the features with low-variance\n",
    "    \"\"\"\n",
    "    fts_geo, fts_int, fts_text = [ 'hu_bin', 'fourier_real', 'fourier_imag'], [ 'stats', 'gabor' ], ['lbp', 'haralick' ]\n",
    "    \n",
    "    reduced_tdata, reduced_vdata = [], []\n",
    "    for fts_group in [ fts_geo, fts_int, fts_text ]:\n",
    "        tdata_fts = [ np.concatenate([ sample[ftname] for ftname in fts_group] ) for sample in tfold_data ]\n",
    "        vdata_fts = [ np.concatenate([ sample[ftname] for ftname in fts_group] ) for sample in vfold_data ]\n",
    "        tdata_fts, vdata_fts = np.stack( tdata_fts ), np.stack( vdata_fts )\n",
    "        tdata_fts[ np.isnan(tdata_fts) ] = 0\n",
    "        vdata_fts[ np.isnan(vdata_fts) ] = 0\n",
    "\n",
    "        # study variance thresholding\n",
    "        thresholding = VarianceThreshold(threshold=thresh)\n",
    "        tdata_reduced = thresholding.fit_transform( tdata_fts )\n",
    "        vdata_reduced = thresholding.transform( vdata_fts )\n",
    "        reduced_tdata.append(tdata_reduced), reduced_vdata.append(vdata_reduced)\n",
    "    return np.concatenate( reduced_tdata, axis=1), np.concatenate( reduced_vdata, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb839ab-e087-4b60-a382-1c979d78ee42",
   "metadata": {},
   "source": [
    "# 1. Support Vector Machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b231216-579a-48e6-8808-2931cb2ce589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "DATA_SUBSETS = [ \n",
    "    'small_without_noise_not_inverted', 'small_with_noise_inverted', 'medium_with_noise_inverted'\n",
    "]\n",
    "\n",
    "# gridSearch for SVM\n",
    "param_grid = {\n",
    "    'C': [1, 5, 10], \n",
    "    'gamma': ['scale', 0.01],\n",
    "    'kernel': ['poly', 'rbf'],\n",
    "    'subset': DATA_SUBSETS\n",
    "}\n",
    "\n",
    "combinations = list( product(*param_grid.values()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c37e45-7551-465a-9413-2d09af83c6a7",
   "metadata": {},
   "source": [
    "GridSearch for the optimal results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f67d9d-145c-4bf5-8703-f2baa01a9036",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing combination:  89%|█████████████████████████████████████▎    | 32/36 [56:25<06:25, 96.28s/it]"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "\n",
    "# make all the combinations for the experiments\n",
    "svm_results = []\n",
    "for pvalues in tqdm( combinations, desc='Testing combination' ):\n",
    "    svm_c, svm_gamma, svm_kernel = pvalues[0], pvalues[1], pvalues[2]\n",
    "    data_subset = pvalues[3]\n",
    "    \n",
    "    fold_generator = generate_group_based_splits( train_file, n_splits=3 )\n",
    "    combination_results = { 'raw': [], 'pca': [], 'var': [] }\n",
    "    for tfold, vfold in fold_generator:\n",
    "        \n",
    "        tfold_data, vfold_data = train_file.iloc[tfold], train_file.iloc[vfold]\n",
    "        tfold_Y, vfold_Y = tfold_data['label'].values, vfold_data['label'].values\n",
    "        tfold_fts, vfold_fts = get_fold_features( tfold_data, vfold_data, subset_name=data_subset )\n",
    "\n",
    "        # raw features\n",
    "        tfold_fts_raw = get_raw_features( tfold_fts )\n",
    "        vfold_fts_raw = get_raw_features( vfold_fts )\n",
    "\n",
    "        # pca/thresholding features\n",
    "        tfold_fts_pca, vfold_fts_pca = apply_group_pca( tfold_fts, vfold_fts )\n",
    "        tfold_fts_var, vfold_fts_var = apply_group_thresholding( tfold_fts, vfold_fts )\n",
    "        \n",
    "        ### Raw Testing\n",
    "        clf = make_pipeline( StandardScaler(), SVC(C=svm_c, gamma=svm_gamma, kernel=svm_kernel, class_weight='balanced') )\n",
    "        clf.fit( tfold_fts_raw, tfold_Y )\n",
    "        vfold_raw_pred = clf.predict( vfold_fts_raw )\n",
    "\n",
    "        raw_acc = clf.score( vfold_fts_raw, vfold_Y )\n",
    "        raw_f1 = f1_score( vfold_Y, vfold_raw_pred, average=None )[ np.where(clf[1].classes_ == 'T')[0][0] ]\n",
    "        raw_f1_macro = f1_score( vfold_Y, vfold_raw_pred, average='macro') # overall, unweighted metric\n",
    "        combination_results['raw'].append( (raw_acc, raw_f1, raw_f1_macro) )\n",
    "        \n",
    "        ### Group-based PCA\n",
    "        clf = make_pipeline( SVC(C=svm_c, gamma=svm_gamma, kernel=svm_kernel, class_weight='balanced') )\n",
    "        clf.fit( tfold_fts_pca, tfold_Y )\n",
    "        vfold_pca_pred = clf.predict( vfold_fts_pca )\n",
    "\n",
    "        pca_acc = clf.score( vfold_fts_pca, vfold_Y )\n",
    "        pca_f1 = f1_score( vfold_Y, vfold_pca_pred, average=None )[ np.where(clf[0].classes_ == 'T')[0][0] ]\n",
    "        pca_f1_macro = f1_score(vfold_Y, vfold_pca_pred, average='macro')\n",
    "        combination_results['pca'].append( (pca_acc, pca_f1, pca_f1_macro) )\n",
    "        \n",
    "        ### Group-based Variance Thrhesholding\n",
    "        clf = make_pipeline( StandardScaler(), SVC(C=svm_c, gamma=svm_gamma, kernel=svm_kernel, class_weight='balanced') )\n",
    "        clf.fit( tfold_fts_var, tfold_Y )\n",
    "        vfold_var_pred = clf.predict( vfold_fts_var )\n",
    "\n",
    "        var_acc = clf.score( vfold_fts_var, vfold_Y )\n",
    "        var_f1 = f1_score( vfold_Y, vfold_var_pred, average=None )[ np.where(clf[1].classes_ == 'T')[0][0] ]\n",
    "        var_f1_macro = f1_score(vfold_Y, vfold_var_pred, average='macro')\n",
    "        combination_results['var'].append( (var_acc, var_f1, var_f1_macro) )\n",
    "        \n",
    "    for rtype in combination_results:\n",
    "        svm_results.append({\n",
    "            'C': svm_c, 'Gamma': svm_gamma, 'Kernel': svm_kernel,\n",
    "            'subset': data_subset,\n",
    "            'Ft_type': rtype, \n",
    "            'acc': np.median( [ v[0] for v in combination_results[rtype] ] ),\n",
    "            'f1_tub': np.median( [ v[1] for v in combination_results[rtype] ] ),\n",
    "            'f1_macro': np.median( [ v[2] for v in combination_results[rtype] ] )\n",
    "        })\n",
    "    \n",
    "svm_results = pd.DataFrame.from_records(svm_results)\n",
    "svm_results.to_csv('svm_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644f7edc-29f9-4b27-ab9e-6eabd9bf9d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m svm_results\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svm_results' is not defined"
     ]
    }
   ],
   "source": [
    "svm_results.sort_values(by=['f1_tub', 'f1_macro'], ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52e91e-4ed3-46de-b626-4c23e204b817",
   "metadata": {},
   "source": [
    "# 2. Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb54a34-351a-488a-9852-e0115995690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "DATA_SUBSETS = [ \n",
    "    'small_without_noise_not_inverted', 'small_with_noise_inverted',\n",
    "    'medium_without_noise_not_inverted', 'medium_with_noise_inverted', \n",
    "    # 'small_with_noise_not_inverted', 'small_without_noise_inverted'\n",
    "]\n",
    "\n",
    "# gridSearch for RandomForest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150], \n",
    "    'subset': DATA_SUBSETS\n",
    "}\n",
    "\n",
    "combinations = list( product(*param_grid.values()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4e1a7-2f9c-46ce-b158-b3cf03e0b1e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Perform the gridseach over the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7e644-81b0-4ddf-9f56-d161b8ec490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# make all the combinations for the experiments\n",
    "rf_results = []\n",
    "for pvalues in tqdm( combinations, desc='Testing combination' ):\n",
    "    rf_estimators = pvalues[0]\n",
    "    data_subset = pvalues[1]\n",
    "    \n",
    "    fold_generator = generate_group_based_splits( train_file, n_splits=4 )\n",
    "    combination_results = { 'raw': [], 'pca': [], 'var': [] }\n",
    "    for tfold, vfold in fold_generator:\n",
    "        \n",
    "        tfold_data, vfold_data = train_file.iloc[tfold], train_file.iloc[vfold]\n",
    "        tfold_Y, vfold_Y = tfold_data['label'].values, vfold_data['label'].values\n",
    "        tfold_fts, vfold_fts = get_fold_features( tfold_data, vfold_data, subset_name=data_subset )\n",
    "\n",
    "        # raw features\n",
    "        tfold_fts_raw = get_raw_features( tfold_fts )\n",
    "        vfold_fts_raw = get_raw_features( vfold_fts )\n",
    "\n",
    "        # pca/thresholding features\n",
    "        tfold_fts_pca, vfold_fts_pca = apply_group_pca( tfold_fts, vfold_fts )\n",
    "        tfold_fts_var, vfold_fts_var = apply_group_thresholding( tfold_fts, vfold_fts )\n",
    "        \n",
    "        ### Raw Testing\n",
    "        clf = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=rf_estimators, class_weight='balanced'))\n",
    "        clf.fit( tfold_fts_raw, tfold_Y )\n",
    "        vfold_raw_pred = clf.predict( vfold_fts_raw )\n",
    "\n",
    "        raw_acc = clf.score( vfold_fts_raw, vfold_Y )\n",
    "        raw_f1 = f1_score( vfold_Y, vfold_raw_pred, average=None )[ np.where(clf[1].classes_ == 'T')[0][0] ]\n",
    "        raw_f1_macro = f1_score( vfold_Y, vfold_raw_pred, average='macro')\n",
    "        combination_results['raw'].append( (raw_acc, raw_f1, raw_f1_macro) )\n",
    "        \n",
    "        ### Group-based PCA\n",
    "        clf = make_pipeline( RandomForestClassifier(n_estimators=rf_estimators, class_weight='balanced'))\n",
    "        clf.fit( tfold_fts_pca, tfold_Y )\n",
    "        vfold_pca_pred = clf.predict( vfold_fts_pca )\n",
    "\n",
    "        pca_acc = clf.score( vfold_fts_pca, vfold_Y )\n",
    "        pca_f1 = f1_score( vfold_Y, vfold_pca_pred, average=None )[ np.where(clf[0].classes_ == 'T')[0][0] ]\n",
    "        pca_f1_macro = f1_score(vfold_Y, vfold_pca_pred, average='macro')\n",
    "        combination_results['pca'].append( (pca_acc, pca_f1, pca_f1_macro) )\n",
    "        \n",
    "        ### Group-based Variance Thrhesholding\n",
    "        clf = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=rf_estimators, class_weight='balanced'))\n",
    "        clf.fit( tfold_fts_var, tfold_Y )\n",
    "        vfold_var_pred = clf.predict( vfold_fts_var )\n",
    "\n",
    "        var_acc = clf.score( vfold_fts_var, vfold_Y )\n",
    "        var_f1 = f1_score( vfold_Y, vfold_var_pred, average=None )[ np.where(clf[1].classes_ == 'T')[0][0] ]\n",
    "        var_f1_macro = f1_score(vfold_Y, vfold_var_pred, average='macro')\n",
    "        combination_results['var'].append( (var_acc, var_f1, var_f1_macro) )\n",
    "        \n",
    "    for rtype in combination_results:\n",
    "        rf_results.append({\n",
    "            'N_Estimators': rf_estimators,\n",
    "            'Ft_type': rtype, \n",
    "            'acc': np.median( [ v[0] for v in combination_results[rtype] ] ),\n",
    "            'f1': np.median( [ v[1] for v in combination_results[rtype] ] ),\n",
    "            'f1_macro': np.median( [ v[2] for v in combination_results[rtype] ] ),\n",
    "            'Subset': data_subset,\n",
    "        })\n",
    "    \n",
    "rf_results = pd.DataFrame.from_records(rf_results)\n",
    "rf_results.to_csv('rf_results.csv', index=False)\n",
    "rf_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
